{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "from transformers import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "scibert_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "scibert_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doctors(string):\n",
    "    pattern = re.compile(r\"Dr.? [A-Z][\\w]+ [A-Z][\\w]+\\b\")\n",
    "    match = re.findall(pattern,string)\n",
    "    if match:\n",
    "        doctor = ' '.join(match[0].split(' ')[-2:])\n",
    "        return doctor\n",
    "    else: return None\n",
    "\n",
    "def extract_relevant_keywords(string):\n",
    "    doc = spacy_nlp(string)\n",
    "    lst_relevant_keywords = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            lst_relevant_keywords.append(token.text)\n",
    "    return list(set(lst_relevant_keywords))\n",
    "\n",
    "def create_dict_expertise(dataframe):\n",
    "    lst_doctors = dataframe['doctors_mentioned'].unique()\n",
    "    dict_expertise = {}\n",
    "    for idx in dataframe.index:\n",
    "        doctor = dataframe.loc[idx,'doctors_mentioned']\n",
    "        if doctor in dict_expertise:\n",
    "            dict_expertise[doctor].update(dataframe.loc[idx,'relevant_keywords'])\n",
    "        else:\n",
    "            dict_expertise[doctor] = set(dataframe.loc[idx,'relevant_keywords'])\n",
    "    return dict_expertise\n",
    "\n",
    "def vectorize_keyword_expertise(dict_expertise):\n",
    "    new_dict = {}\n",
    "    for key,values in dict_expertise.items():\n",
    "        lst_tuples = []\n",
    "        for keyword in values:\n",
    "            token_keyword = torch.tensor(scibert_tokenizer.encode(keyword)).unsqueeze(0)\n",
    "            out = scibert_model(token_keyword)\n",
    "            word_vector = out[0][0][1]\n",
    "            tup = (keyword,word_vector)\n",
    "            lst_tuples.append(tup)\n",
    "        all_expertise_vectors = np.array([value[1].detach().numpy() for value in lst_tuples])\n",
    "        mean_expertise_vector = np.mean(all_expertise_vectors,axis=0)\n",
    "        new_dict[key] = {'average_expertise_vector':mean_expertise_vector,'keywords_expertise':lst_tuples}\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv('articles_data.csv',index_col=0)\n",
    "articles_df['doctors_mentioned'] = articles_df['body'].apply(extract_doctors)\n",
    "articles_df = articles_df[articles_df[\"doctors_mentioned\"].notnull()]\n",
    "articles_df = articles_df[:50] #pour éviter que ça ne ralentisse trop\n",
    "articles_df['relevant_keywords'] = articles_df['keywords'].apply(extract_relevant_keywords)\n",
    "dict_expertise = create_dict_expertise(articles_df)\n",
    "dict_expertise_with_average = vectorize_keyword_expertise(dict_expertise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthony Fauci ['animals', 'COVID-19', 'host', 'reservoir', 'need', 'press', 'virus', 'president', 'humans', 'people', 'infection', 'origins', 'vaccine', 'diseases', 'experts', 'report', 'vaccinations', 'outlaw', 'coronavirus', 'secretary', 'transmission']\n"
     ]
    }
   ],
   "source": [
    "search_terms = 'climate change caused by chem trails'\n",
    "\n",
    "token_keyword = torch.tensor(scibert_tokenizer.encode(search_terms)).unsqueeze(0)\n",
    "out = scibert_model(token_keyword)\n",
    "embeddings = out[0][0][1:-1]\n",
    "average_embedding_request = np.mean(embeddings.detach().numpy(),axis=0)\n",
    "\n",
    "max_cos_similarity = 0\n",
    "corresponding_key = ''\n",
    "for key,value in dict_expertise_with_average.items():\n",
    "    cos_similarity = cosine_similarity(average_embedding_request.reshape(1,-1),value['average_expertise_vector'].reshape(1,-1))[0][0]\n",
    "    if cos_similarity > max_cos_similarity:\n",
    "        max_cos_similarity = cos_similarity\n",
    "        corresponding_key = key\n",
    "        \n",
    "print(key, [elem[0] for elem in dict_expertise_with_average[key]['keywords_expertise']])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
