{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Launch cell to collect a database of 1100 articles categorized as 'science' from \n",
    "(more or less) reliable sources and save it to a dataframe\n",
    "'''\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def make_call(cursor):\n",
    "    headers = {'X-Application-Id' : 'XXX',\n",
    "               'X-Application-Key': 'XXX'\n",
    "                }\n",
    "\n",
    "# application-id: 789284d + la lettre a ajoutée à la fin\n",
    "# clé: c819ebf4b24cd9bf8c63ad754130a6a + la lettre c ajoutée à la fin\n",
    "\n",
    "    params = {\n",
    "            'language':  ['en'],\n",
    "            'body' : ' Dr.',\n",
    "            'published_at.start' : '2021-01-15T00:00:00Z',\n",
    "            'published_at.end' : '2021-04-07T00:00:00Z',\n",
    "            'categories.taxonomy' : 'iptc-subjectcode',\n",
    "            'categories.id':'13000000', #corresponds to the 'science' category\n",
    "            'source_rankings_alexa_rank_min': 300,\n",
    "            #'source.name' : ['cnn','bbc'],\n",
    "            'social_shares_count_linkedin_min': 10000,\n",
    "            'source_links_in_count_min': 10000,\n",
    "            'cursor' : cursor,\n",
    "            'per_page' : 100\n",
    "            }\n",
    "\n",
    "    url = 'https://api.aylien.com/news/stories'\n",
    "    r = requests.get(url,headers=headers,params=params)\n",
    "    return r.json()\n",
    "\n",
    "#initialize call\n",
    "response = make_call('*')\n",
    "next_page = response[\"next_page_cursor\"]\n",
    "all_stories = response[\"stories\"]\n",
    "lst_sources = []\n",
    "\n",
    "#iterate through chunks of 100 articles \n",
    "for page in range(10):\n",
    "    response = make_call(next_page)\n",
    "    next_page = response[\"next_page_cursor\"]\n",
    "    stories = response[\"stories\"]\n",
    "    all_stories += stories\n",
    "\n",
    "#select only data of preliminary interest and append it to dataframe\n",
    "df = pd.DataFrame(columns = ['body','title','source','publication_date','words_count','hashtags','keywords'])\n",
    "for story in all_stories:\n",
    "    mini_dict = {'body':story['body'],\n",
    "                'title':story['title'],\n",
    "                 'source':story['source']['name'],\n",
    "                 'publication_date':story['published_at'],\n",
    "                 'words_count':story['words_count'],\n",
    "                 'hashtags': str(story['hashtags']).replace('#','').replace(\"\\'\",'').replace('[','').replace(']',''),\n",
    "                 'keywords': str(story['keywords']).replace(\"\\'\",'').replace('[','').replace(']','')\n",
    "                }\n",
    "\n",
    "    df = df.append(mini_dict,ignore_index=True)\n",
    "\n",
    "df.to_csv('articles_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install spacy\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "#!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "from transformers import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "scibert_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "scibert_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doctors(string):\n",
    "    pattern = re.compile(r\"Dr.? [A-Z][\\w]+ [A-Z][\\w]+\\b\")\n",
    "    match = re.findall(pattern,string)\n",
    "    if match:\n",
    "        doctor = ' '.join(match[0].split(' ')[-2:])\n",
    "        return doctor\n",
    "    else: return None\n",
    "\n",
    "def extract_relevant_keywords(string):\n",
    "    doc = spacy_nlp(string)\n",
    "    lst_relevant_keywords = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            lst_relevant_keywords.append(token.text)\n",
    "    return list(set(lst_relevant_keywords))\n",
    "\n",
    "def create_dict_expertise(dataframe):\n",
    "    lst_doctors = dataframe['doctors_mentioned'].unique()\n",
    "    dict_expertise = {}\n",
    "    for idx in dataframe.index:\n",
    "        doctor = dataframe.loc[idx,'doctors_mentioned']\n",
    "        if doctor in dict_expertise:\n",
    "            dict_expertise[doctor].update(dataframe.loc[idx,'relevant_keywords'])\n",
    "        else:\n",
    "            dict_expertise[doctor] = set(dataframe.loc[idx,'relevant_keywords'])\n",
    "    return dict_expertise\n",
    "\n",
    "def vectorize_keyword_expertise(dict_expertise):\n",
    "    new_dict = {}\n",
    "    for key,values in dict_expertise.items():\n",
    "        lst_tuples = []\n",
    "        for keyword in values:\n",
    "            token_keyword = torch.tensor(scibert_tokenizer.encode(keyword)).unsqueeze(0)\n",
    "            out = scibert_model(token_keyword)\n",
    "            word_vector = out[0][0][1]\n",
    "            tup = (keyword,word_vector)\n",
    "            lst_tuples.append(tup)\n",
    "        all_expertise_vectors = np.array([value[1].detach().numpy() for value in lst_tuples])\n",
    "        mean_expertise_vector = np.mean(all_expertise_vectors,axis=0)\n",
    "        new_dict[key] = {'average_expertise_vector':mean_expertise_vector,'keywords_expertise':lst_tuples}\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv('articles_data.csv',index_col=0)\n",
    "articles_df['doctors_mentioned'] = articles_df['body'].apply(extract_doctors)\n",
    "articles_df = articles_df[articles_df[\"doctors_mentioned\"].notnull()]\n",
    "articles_df = articles_df[:50] #pour éviter que ça ne ralentisse trop\n",
    "articles_df['relevant_keywords'] = articles_df['keywords'].apply(extract_relevant_keywords)\n",
    "dict_expertise = create_dict_expertise(articles_df)\n",
    "dict_expertise_with_average = vectorize_keyword_expertise(dict_expertise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = 'climate change caused by chem trails'\n",
    "\n",
    "token_keyword = torch.tensor(scibert_tokenizer.encode(search_terms)).unsqueeze(0)\n",
    "out = scibert_model(token_keyword)\n",
    "embeddings = out[0][0][1:-1]\n",
    "average_embedding_request = np.mean(embeddings.detach().numpy(),axis=0)\n",
    "\n",
    "max_cos_similarity = 0\n",
    "corresponding_key = ''\n",
    "for key,value in dict_expertise_with_average.items():\n",
    "    cos_similarity = cosine_similarity(average_embedding_request.reshape(1,-1),value['average_expertise_vector'].reshape(1,-1))[0][0]\n",
    "    if cos_similarity > max_cos_similarity:\n",
    "        max_cos_similarity = cos_similarity\n",
    "        corresponding_key = key\n",
    "        \n",
    "print(key, [elem[0] for elem in dict_expertise_with_average[key]['keywords_expertise']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
